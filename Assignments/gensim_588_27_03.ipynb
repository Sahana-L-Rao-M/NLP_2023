{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**GENSIM EXPLORATION**\n",
        "\n",
        "**NAME :** SAHANA RAO\n",
        "\n",
        "**SRN:** PES1UG20CS588\n",
        "\n",
        "**SECTION:** J"
      ],
      "metadata": {
        "id": "I2FuNBvTS7s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryGsqZEY2Nlv",
        "outputId": "f420dc4f-9872-4e99-8bc0-8465e0bc1b00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenising text using Gensim** "
      ],
      "metadata": {
        "id": "Va4C5-4Ga9_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Preprocessing\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "text = \"My name is Sahana Rao.\"\n",
        "tokens = list(tokenize(text))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US7W2nXjOftk",
        "outputId": "da58bb36-c880-4c71-efe0-4b1d1f3deef9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'name', 'is', 'Sahana', 'Rao']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SIMILARITY RETRIVAL USING GENSIM**\n",
        "\n",
        "It is a  process of finding documents or words that are similar to a given query document or word based on some similarity metric. "
      ],
      "metadata": {
        "id": "Eb-5Hx6jayjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models, similarities\n",
        "\n",
        "# Create a corpus of documents\n",
        "corpus = [\n",
        "    ['rose', 'lily', 'tulip'],\n",
        "    ['tulip', 'garland'],\n",
        "    ['rose', 'garland', 'tulip', 'hibiscus'],\n",
        "    ['lily', 'garland']\n",
        "]\n",
        "\n",
        "# Create a dictionary from the corpus\n",
        "dictionary = corpora.Dictionary(corpus)\n",
        "\n",
        "# Convert the corpus into a bag-of-words representation\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in corpus]\n",
        "\n",
        "# Train a TF-IDF model on the corpus\n",
        "tfidf_model = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "\n",
        "matrix_sim = similarities.MatrixSimilarity(tfidf_corpus)\n",
        "sparse_sim = similarities.SparseMatrixSimilarity(tfidf_corpus, num_features=len(dictionary))\n",
        "\n",
        "# Compute the similarities between the documents\n",
        "query = ['rose', 'garland']\n",
        "query_bow = dictionary.doc2bow(query)\n",
        "query_tfidf = tfidf_model[query_bow]\n",
        "\n",
        "\n",
        "sims1 = matrix_sim[query_tfidf]\n",
        "print(list(enumerate(sims1)))"
      ],
      "metadata": {
        "id": "RvcG6F7V2ZA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcce40f5-b9d6-4053-9bdd-a00cbc9e1878"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.6266618), (1, 0.27105728), (2, 0.46833566), (3, 0.14694409)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**models.Doc2Vec**\n",
        "\n",
        "models.Doc2Vec.most_similar is a method in the Doc2Vec model class in Gensim that returns the top-N most similar documents to a given query document based on their cosine similarity scores in the vector space model."
      ],
      "metadata": {
        "id": "KL7Z20jCa6rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2jJoNOaQgYK",
        "outputId": "bc11a37c-c223-4044-9794-9125f832e741"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog is very lazy.\",\n",
        "    \"Hello world.\"\n",
        "]\n",
        "\n",
        "\n",
        "tagged_documents = [TaggedDocument(simple_preprocess(doc), [i]) for i, doc in enumerate(documents)]\n",
        "\n",
        "# Train a Doc2Vec model on the TaggedDocument objects\n",
        "model = Doc2Vec(tagged_documents, vector_size=100, window=5, min_count=1, epochs=50)\n",
        "\n",
        "\n",
        "query_doc = \"The brown fox is quick and the dog is lazy.\"\n",
        "query_vector = model.infer_vector(simple_preprocess(query_doc))\n",
        "\n",
        "\n",
        "similar_docs = model.docvecs.most_similar(positive=[query_vector], topn=2)\n",
        "\n",
        "\n",
        "for doc_id, similarity in similar_docs:\n",
        "    print(documents[doc_id])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl4XF1O5N7Qr",
        "outputId": "d9a38ea0-1a0d-45b8-c5d2-ed56a15a20d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog.\n",
            "Hello world.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5a892a29e91d>:22: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_docs = model.docvecs.most_similar(positive=[query_vector], topn=2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**models.Word2Vec.most_similar**\n",
        "\n",
        "This method returns the most similar words to a given word in a pre-trained Word2Vec model. It computes cosine similarity between the word's vector representation and the vectors of all other words in the model."
      ],
      "metadata": {
        "id": "sGsuzpoia7k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# define a list of sentences\n",
        "sentences = [[\"this\", \"is\", \"the\", \"first\", \"sentence\", \"for\", \"word2vec\"],\n",
        "             [\"this\", \"is\", \"the\", \"second\", \"sentence\"],\n",
        "             [\"yet\", \"another\", \"sentence\"],\n",
        "             [\"one\", \"more\", \"sentence\"],\n",
        "             [\"and\", \"the\", \"final\", \"sentence\"]]\n",
        "\n",
        "# train a Word2Vec model on the sentences\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, epochs=50)\n",
        "\n",
        "# find the most similar words to a given word\n",
        "similar_words = model.wv.most_similar(\"sentence\",topn=5)\n",
        "print(similar_words)\n",
        "for word, similarity in similar_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532LpOoGObci",
        "outputId": "6ea33dc4-20fc-41e9-87d9-56c00d535161"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('yet', 0.21735575795173645), ('for', 0.09428801387548447), ('one', 0.09294721484184265), ('word2vec', 0.08002333343029022), ('second', 0.0633990541100502)]\n",
            "yet\n",
            "for\n",
            "one\n",
            "word2vec\n",
            "second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling**\n",
        "\n",
        "Topic modeling is a natural language processing technique that allows you to identify the main topics or themes in a collection of documents. Gensim provides a simple and powerful way to perform topic modeling using the Latent Dirichlet Allocation (LDA) algorithm."
      ],
      "metadata": {
        "id": "0JPZYsHbbDL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data.\",\n",
        "    \"Natural language processing is a field of study focused on making computers understand human language.\",\n",
        "    \"Deep learning is a subset of machine learning that uses neural networks with many layers to model complex data.\",\n",
        "    \"Computer vision is the field of study focused on teaching machines to interpret and understand visual data.\",\n",
        "    \"Reinforcement learning is a type of machine learning that trains agents to take actions in an environment to maximize a reward.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in corpus])\n",
        "\n",
        "# Create bag of words corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc.split()) for doc in corpus]\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = gensim.models.LdaModel(\n",
        "    corpus=bow_corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=3, \n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Print topics\n",
        "pprint(lda_model.print_topics())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ8a2_SYOlHN",
        "outputId": "26ba5e46-e354-49d6-a32d-5bb5e7d1e329"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.037*\"machines\" + 0.037*\"vision\" + 0.037*\"study\" + 0.037*\"focused\" + '\n",
            "  '0.037*\"field\" + 0.037*\"of\" + 0.037*\"interpret\" + 0.037*\"data.\" + '\n",
            "  '0.037*\"Computer\" + 0.037*\"teaching\"'),\n",
            " (1,\n",
            "  '0.069*\"learning\" + 0.056*\"a\" + 0.056*\"that\" + 0.043*\"to\" + 0.043*\"of\" + '\n",
            "  '0.043*\"is\" + 0.030*\"machine\" + 0.030*\"data.\" + 0.030*\"subset\" + '\n",
            "  '0.017*\"Reinforcement\"'),\n",
            " (2,\n",
            "  '0.039*\"on\" + 0.039*\"is\" + 0.039*\"of\" + 0.039*\"understand\" + 0.039*\"field\" + '\n",
            "  '0.039*\"focused\" + 0.039*\"a\" + 0.039*\"study\" + 0.039*\"computers\" + '\n",
            "  '0.039*\"human\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer topics for new document\n",
        "new_doc = \"Artificial intelligence is the field of study focused on building intelligent machines.\"\n",
        "\n",
        "new_doc_bow = dictionary.doc2bow(new_doc.split())\n",
        "new_doc_topics, word_topics, phi_values = lda_model.get_document_topics(new_doc_bow, per_word_topics=True)\n",
        "pprint(new_doc_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub7T-7mn2T4c",
        "outputId": "e543c852-a280-4267-c8eb-e2c690787052"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.7400925), (1, 0.25486773)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word Embeddings**\n",
        "\n",
        "Word embeddings are a powerful technique for representing words as dense vectors in a high-dimensional space. Gensim provides an easy-to-use implementation of several popular word embedding algorithms, including Word2Vec and FastText. "
      ],
      "metadata": {
        "id": "OMeTfUAZbFtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings\n",
        "import gensim\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data.\",\n",
        "    \"Natural language processing is a field of study focused on making computers understand human language.\",\n",
        "    \"Deep learning is a subset of machine learning that uses neural networks with many layers to model complex data.\",\n",
        "    \"Computer vision is the field of study focused on teaching machines to interpret and understand visual data.\",\n",
        "    \"Reinforcement learning is a type of machine learning that trains agents to take actions in an environment to maximize a reward.\"\n",
        "]\n",
        "\n",
        "# Preprocessing the corpus\n",
        "preprocessed_corpus = [doc.split() for doc in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = gensim.models.Word2Vec(preprocessed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word embeddings\n",
        "print(model.wv['Machine'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY930En2OqBP",
        "outputId": "a6ebe750-5db9-4e40-e1f8-5c8d3778fe81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-6.4630997e-03  7.3367576e-03  6.1177979e-03 -4.9100812e-03\n",
            " -1.7541874e-03 -2.5121467e-03  3.1211844e-03 -4.5212783e-04\n",
            " -2.7682818e-03 -9.0516107e-03  6.4337696e-03 -9.6248072e-03\n",
            " -8.6518833e-03  1.3884846e-03  2.9792865e-03 -7.4751347e-07\n",
            "  3.8094976e-04  2.5881811e-03  1.8030650e-03  7.5679389e-03\n",
            " -3.4414798e-03 -7.2295824e-03 -7.8792162e-03  7.9229185e-03\n",
            "  1.9487789e-03 -5.8045038e-03  6.3820719e-03  8.4539428e-03\n",
            "  7.9488838e-03 -6.8302597e-03 -5.1375949e-03 -2.0091264e-03\n",
            " -6.6235662e-03  4.7205924e-03  5.6654154e-03 -5.9954687e-03\n",
            "  7.2007733e-03 -7.6522226e-03  6.2051262e-03 -4.6260231e-03\n",
            "  1.9630603e-03 -3.2784708e-03  3.3939371e-03  7.7177049e-03\n",
            " -1.3956031e-03 -4.9823239e-03 -8.5704457e-03  3.6361234e-03\n",
            "  4.2371978e-03  6.6932952e-03 -4.2884005e-03 -9.7401831e-03\n",
            "  6.8214135e-03 -4.5232675e-03  4.4192504e-03  8.0564367e-03\n",
            " -3.9324570e-03  1.6356074e-03 -3.5863242e-03 -6.6093006e-03\n",
            " -5.6845113e-03 -1.3819299e-04 -7.5054220e-03 -1.9887215e-03\n",
            " -2.4843570e-03  6.7112739e-03  1.3601339e-03  5.8561843e-03\n",
            "  3.8158973e-03 -3.5140603e-03 -5.0881626e-03  9.6619297e-03\n",
            "  7.0741009e-03 -9.0432866e-03  5.2516027e-03  8.1923408e-03\n",
            "  9.1887526e-03 -8.0483537e-03 -2.0152063e-03  1.0814901e-03\n",
            "  2.8995101e-03 -1.6240022e-03 -4.7022561e-03  6.4704837e-03\n",
            "  6.9373921e-03 -3.9054709e-04 -5.8392207e-03 -4.3645673e-03\n",
            " -5.1450455e-03  4.2544943e-03  5.2246929e-04 -2.3556412e-03\n",
            " -9.0849875e-03 -1.4994337e-03  6.4348131e-03  6.3301288e-03\n",
            "  4.2312006e-03 -2.8809041e-03  9.0456400e-03  6.0527574e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**corpora.Dictionary**\n",
        "\n",
        " The corpora.Dictionary class in gensim is a utility for creating and manipulating a mapping between words and their integer ids. This is useful for many natural language processing tasks, including topic modeling and text classification."
      ],
      "metadata": {
        "id": "QFaXmBACOuWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus to Dictionary\n",
        "from gensim import corpora\n",
        "\n",
        "# List of documents\n",
        "documents = ['rose is a flower', 'tulip is also a flower', 'Frog is an animal']\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in documents])\n",
        "\n",
        "# Print dictionary\n",
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWiu1SVUOtvd",
        "outputId": "84903f72-fee3-4d76-e302-9444c9018786"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'flower': 1, 'is': 2, 'rose': 3, 'also': 4, 'tulip': 5, 'Frog': 6, 'an': 7, 'animal': 8}\n"
          ]
        }
      ]
    }
  ]
}